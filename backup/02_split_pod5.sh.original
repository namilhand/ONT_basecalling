#!/bin/bash

# ==============================================================================
# POD5 Split - Keep Only Non-Duplicated Reads
# ==============================================================================
# Strategy: Only process reads that appear EXACTLY ONCE in the original file
# This avoids the duplicate issue entirely
# ==============================================================================

# To keep log, (not sure if this redirection correct)
# bash ${input_pod5} ${reads_per_chunk} ${output_dir} 2>& pod5_split.log

set -e
set -u

INPUT_POD5="${1:-SAM-seq5.pod5}"
READS_PER_CHUNK="${2:-1000000}"
OUTPUT_DIR="${3:-pod5_chunks}"

echo "========================================================================"
echo "POD5 Split - Non-Duplicated Reads Only"
echo "========================================================================"
echo "Input: $INPUT_POD5"
echo "Output: $OUTPUT_DIR"
echo "Strategy: Only keep reads that appear EXACTLY once"
echo "========================================================================"
echo ""

BASENAME=$(basename "$INPUT_POD5" .pod5)
TEMP_DIR=".temp_nodups_$$"
mkdir -p "$TEMP_DIR"
mkdir -p "$OUTPUT_DIR"

# Step 1: Find read IDs that appear EXACTLY once
echo "[1/4] Analyzing read ID frequencies..."
ALL_IDS="${TEMP_DIR}/all_ids.txt"
pod5 view "$INPUT_POD5" --ids --no-header > "$ALL_IDS"

TOTAL=$(wc -l < "$ALL_IDS")
echo "  Total read entries: $(printf "%'d" $TOTAL)"

echo "  Finding reads that appear exactly once..."
# This awk command counts occurrences and only prints IDs that appear once
SINGLETON_IDS="${TEMP_DIR}/singleton_ids.txt"

awk '{count[$1]++} END {
    for (id in count) {
        if (count[id] == 1) {
            print id
        }
    }
}' "$ALL_IDS" | sort > "$SINGLETON_IDS"

SINGLETON_COUNT=$(wc -l < "$SINGLETON_IDS")
DUPLICATE_READS=$((TOTAL - SINGLETON_COUNT))

echo "  Reads appearing exactly once: $(printf "%'d" $SINGLETON_COUNT)"
echo "  Reads to be excluded (appear >1 time): $(printf "%'d" $DUPLICATE_READS)"
echo ""

# Find which IDs are duplicated (for reporting)
echo "  Identifying duplicated read IDs..."
DUPLICATE_IDS="${TEMP_DIR}/duplicate_ids.txt"

awk '{count[$1]++; ids[$1]} END {
    for (id in count) {
        if (count[id] > 1) {
            print id, count[id]
        }
    }
}' "$ALL_IDS" | sort > "$DUPLICATE_IDS"

NUM_DUP_IDS=$(wc -l < "$DUPLICATE_IDS")
echo "  Number of unique read IDs that are duplicated: $(printf "%'d" $NUM_DUP_IDS)"

echo ""
echo "  Sample of duplicated read IDs (first 10):"
head -10 "$DUPLICATE_IDS" | while read id count; do
    echo "    $id (appears $count times - will be excluded)"
done

echo ""
echo "Proceeding with $SINGLETON_COUNT non-duplicated reads..."
echo ""

# Step 2: Split singleton IDs into batches
echo "[2/4] Creating batches of singleton read IDs..."

BATCH_SIZE=50000
split -l $BATCH_SIZE -d -a 4 "$SINGLETON_IDS" "${TEMP_DIR}/batch_"

BATCH_FILES=(${TEMP_DIR}/batch_*)
NUM_BATCHES=${#BATCH_FILES[@]}

echo "  Created $NUM_BATCHES batches of ~$BATCH_SIZE reads each"
echo ""

# Step 3: Extract reads using pod5 subset
echo "[3/4] Extracting non-duplicated reads..."
echo "  This should work cleanly since no ID appears more than once"
echo ""

MINI_DIR="${TEMP_DIR}/mini_chunks"
mkdir -p "$MINI_DIR"

BATCH_NUM=0
START_TIME=$(date +%s)

for BATCH_FILE in "${BATCH_FILES[@]}"; do
    BATCH_NUM=$((BATCH_NUM + 1))
    BATCH_SUFFIX=$(basename "$BATCH_FILE" | sed 's/batch_//')
    
    NUM_READS=$(wc -l < "$BATCH_FILE")
    PERCENT=$((BATCH_NUM * 100 / NUM_BATCHES))
    
    printf "  [%3d%%] Batch %04d/%04d (%'d reads)... " \
           $PERCENT $BATCH_NUM $NUM_BATCHES $NUM_READS
    
    # Create CSV mapping
    MAPPING="${TEMP_DIR}/map_${BATCH_SUFFIX}.csv"
    OUTPUT_NAME="mini_${BATCH_SUFFIX}.pod5"
    
    echo "target,read_id" > "$MAPPING"
    while read -r read_id; do
        echo "${OUTPUT_NAME},${read_id}" >> "$MAPPING"
    done < "$BATCH_FILE"
    
    # Run pod5 subset (should work perfectly now)
    if timeout 600 pod5 subset "$INPUT_POD5" \
           --csv "$MAPPING" \
           --output "$MINI_DIR" 2>&1 | grep -qi "error\|duplicate\|segmentation"; then
        echo "FAILED"
        echo "  Unexpected error - check if input file has issues"
        rm -rf "$TEMP_DIR"
        exit 1
    fi
    
    MINI_OUTPUT="${MINI_DIR}/${OUTPUT_NAME}"
    
    if [ -f "$MINI_OUTPUT" ]; then
        # Verify
        MINI_TOTAL=$(pod5 view "$MINI_OUTPUT" --ids --no-header 2>/dev/null | wc -l)
        MINI_UNIQUE=$(pod5 view "$MINI_OUTPUT" --ids --no-header 2>/dev/null | sort -u | wc -l)
        SIZE=$(du -h "$MINI_OUTPUT" | cut -f1)
        
        if [ $MINI_TOTAL -eq $MINI_UNIQUE ] && [ $MINI_TOTAL -eq $NUM_READS ]; then
            echo "✓ Perfect (${SIZE}, $MINI_TOTAL reads)"
        elif [ $MINI_TOTAL -eq $MINI_UNIQUE ]; then
            echo "OK (${SIZE}, $MINI_TOTAL reads, expected $NUM_READS)"
        else
            echo "WARNING (${SIZE}, $MINI_TOTAL total, $MINI_UNIQUE unique)"
        fi
    else
        echo "FAILED (no output)"
        rm -rf "$TEMP_DIR"
        exit 1
    fi
    
    rm -f "$MAPPING"
    
    # Progress estimate
    if [ $((BATCH_NUM % 10)) -eq 0 ]; then
        CURRENT=$(date +%s)
        ELAPSED=$((CURRENT - START_TIME))
        AVG=$((ELAPSED / BATCH_NUM))
        REMAIN=$((NUM_BATCHES - BATCH_NUM))
        EST=$((AVG * REMAIN / 60))
        
        if [ $EST -gt 0 ]; then
            echo "       (Est. remaining: ${EST} min)"
        fi
    fi
done

MINI_TIME=$(date +%s)
MINI_ELAPSED=$((MINI_TIME - START_TIME))

echo ""
echo "  Mini-chunk creation: $((MINI_ELAPSED / 60))m $((MINI_ELAPSED % 60))s"

# Verify all mini-chunks
MINI_FILES=(${MINI_DIR}/mini_*.pod5)
echo "  Created ${#MINI_FILES[@]} mini-chunk files"

echo ""
echo "  Final verification of mini-chunks..."
VERIFY_TOTAL=0
for mini in ${MINI_DIR}/mini_*.pod5; do
    COUNT=$(pod5 view "$mini" --ids --no-header 2>/dev/null | wc -l)
    VERIFY_TOTAL=$((VERIFY_TOTAL + COUNT))
done

echo "  Total reads in mini-chunks: $(printf "%'d" $VERIFY_TOTAL)"
echo "  Expected singleton reads: $(printf "%'d" $SINGLETON_COUNT)"

if [ $VERIFY_TOTAL -eq $SINGLETON_COUNT ]; then
    echo "  ✓ Perfect match!"
else
    echo "  ⚠️  Mismatch: difference of $(($SINGLETON_COUNT - $VERIFY_TOTAL)) reads"
fi

echo ""

# Step 4: Merge into final chunks
echo "[4/4] Merging mini-chunks into final chunks..."
echo ""

MINIS_PER_FINAL=$((READS_PER_CHUNK / BATCH_SIZE + 1))
FINAL_CHUNK=0
MINI_IDX=0

while [ $MINI_IDX -lt ${#MINI_FILES[@]} ]; do
    FINAL_CHUNK=$((FINAL_CHUNK + 1))
    
    # Collect minis for this final chunk
    CHUNK_MINIS=()
    for ((i=0; i<MINIS_PER_FINAL && MINI_IDX<${#MINI_FILES[@]}; i++)); do
        CHUNK_MINIS+=("${MINI_FILES[$MINI_IDX]}")
        MINI_IDX=$((MINI_IDX + 1))
    done
    
    FINAL_OUTPUT="${OUTPUT_DIR}/${BASENAME}_chunk$(printf "%03d" $FINAL_CHUNK).pod5"
    
    printf "  Chunk %03d: Merging %d mini-chunks... " \
           $FINAL_CHUNK ${#CHUNK_MINIS[@]}
    
    if [ ${#CHUNK_MINIS[@]} -eq 1 ]; then
        cp "${CHUNK_MINIS[0]}" "$FINAL_OUTPUT"
    else
        if pod5 merge "${CHUNK_MINIS[@]}" --output "$FINAL_OUTPUT" 2>&1 | \
           grep -qi "error"; then
            echo "FAILED"
            rm -rf "$TEMP_DIR"
            exit 1
        fi
    fi
    
    if [ -f "$FINAL_OUTPUT" ]; then
        SIZE=$(du -h "$FINAL_OUTPUT" | cut -f1)
        NREADS=$(pod5 view "$FINAL_OUTPUT" --ids --no-header | wc -l)
        NUNIQUE=$(pod5 view "$FINAL_OUTPUT" --ids --no-header | sort -u | wc -l)
        
        if [ $NREADS -eq $NUNIQUE ]; then
            echo "✓ Perfect (${SIZE}, $(printf "%'d" $NREADS) unique reads)"
        else
            echo "⚠️  WARNING (${SIZE}, $NREADS total, $NUNIQUE unique)"
        fi
    else
        echo "FAILED"
        rm -rf "$TEMP_DIR"
        exit 1
    fi
done

MERGE_TIME=$(date +%s)
MERGE_ELAPSED=$((MERGE_TIME - MINI_TIME))
TOTAL_ELAPSED=$((MERGE_TIME - START_TIME))

echo ""
echo "  Merge time: $((MERGE_ELAPSED / 60))m $((MERGE_ELAPSED % 60))s"

# Cleanup
echo ""
echo "Cleaning up temporary files..."
rm -rf "$TEMP_DIR"

echo ""
echo "========================================================================"
echo "COMPLETE!"
echo "========================================================================"
echo "Total time: $((TOTAL_ELAPSED / 60))m $((TOTAL_ELAPSED % 60))s"
echo ""

NUM_CHUNKS=$(ls "$OUTPUT_DIR"/${BASENAME}_chunk*.pod5 2>/dev/null | wc -l)
echo "Results:"
echo "  Input file: $INPUT_POD5"
echo "  Total reads in input: $(printf "%'d" $TOTAL)"
echo "  Singleton reads (used): $(printf "%'d" $SINGLETON_COUNT)"
echo "  Duplicated reads (excluded): $(printf "%'d" $DUPLICATE_READS)"
echo "  Output chunks created: $NUM_CHUNKS"
echo ""

# Final verification
echo "Final chunk verification:"
TOTAL_OUTPUT=0
ALL_CLEAN=1

for chunk in "$OUTPUT_DIR"/${BASENAME}_chunk*.pod5; do
    if [ -f "$chunk" ]; then
        CHUNK_TOTAL=$(pod5 view "$chunk" --ids --no-header 2>/dev/null | wc -l)
        CHUNK_UNIQUE=$(pod5 view "$chunk" --ids --no-header 2>/dev/null | sort -u | wc -l)
        CHUNK_NAME=$(basename "$chunk")
        SIZE=$(du -h "$chunk" | cut -f1)
        
        TOTAL_OUTPUT=$((TOTAL_OUTPUT + CHUNK_TOTAL))
        
        if [ $CHUNK_TOTAL -eq $CHUNK_UNIQUE ]; then
            printf "  ✓ %-35s %8s  %'10d unique reads\n" "$CHUNK_NAME" "$SIZE" "$CHUNK_TOTAL"
        else
            printf "  ✗ %-35s %8s  %'10d total, %'10d unique (ERROR!)\n" \
                   "$CHUNK_NAME" "$SIZE" "$CHUNK_TOTAL" "$CHUNK_UNIQUE"
            ALL_CLEAN=0
        fi
    fi
done

echo ""
echo "Total reads in all chunks: $(printf "%'d" $TOTAL_OUTPUT)"

if [ $ALL_CLEAN -eq 1 ] && [ $TOTAL_OUTPUT -eq $SINGLETON_COUNT ]; then
    echo ""
    echo "✓✓✓ SUCCESS! All chunks are clean with only non-duplicated reads ✓✓✓"
    echo ""
    echo "========================================================================"
    echo "Next steps:"
    echo "  1. Update dorado_basecalling_array.sh:"
    echo "     #SBATCH --array=1-${NUM_CHUNKS}%1"
    echo ""
    echo "  2. Submit basecalling:"
    echo "     sbatch dorado_basecalling_array.sh"
    echo ""
    echo "Note: Basecalling will use $(printf "%'d" $SINGLETON_COUNT) reads"
    echo "      ($(printf "%'d" $DUPLICATE_READS) duplicated reads were excluded)"
    echo "========================================================================"
else
    echo ""
    echo "⚠️  WARNING: Issues detected in output"
    
    if [ $ALL_CLEAN -eq 0 ]; then
        echo "  - Some chunks contain duplicates"
    fi
    
    if [ $TOTAL_OUTPUT -ne $SINGLETON_COUNT ]; then
        echo "  - Read count mismatch"
    fi
fi
